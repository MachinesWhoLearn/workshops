{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import zipfile\n",
    "import csv\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "# It is necessary to download spacy data by running:\n",
    "# \"python -m spacy download en\" in your shell.\n",
    "import spacy\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We're assuming the data is at ./data/raw/train.csv\n",
    "# If it is not there, you will want to change this variable\n",
    "# to indicate the correct path\n",
    "quora_data_path = \"./data/raw/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404290it [00:51, 7820.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_lines = []\n",
    "\n",
    "with open(quora_data_path, \"r\") as quora_data:\n",
    "    data_reader = csv.reader(quora_data)\n",
    "    # skip the header\n",
    "    header = next(data_reader, None)\n",
    "    # iterate over the lines in the CSV reader (around 400,000)\n",
    "    for line in tqdm(data_reader):\n",
    "        question_1 = line[3]\n",
    "        question_2 = line[4]\n",
    "        is_duplicate = line[5]\n",
    "        \n",
    "        # tokenize the questions (break it up into words) with SpaCy\n",
    "        # Make sure all the tokens are lowercased.\n",
    "        question_1_tokens = [str(token).lower() for token in en_nlp.tokenizer(question_1)]\n",
    "        question_2_tokens = [str(token).lower() for token in en_nlp.tokenizer(question_2)]\n",
    "        \n",
    "        # add a tuple of (tokenized questions, label (as an int))\n",
    "        # to the list of cleaned lines.\n",
    "        cleaned_lines.append((question_1_tokens, question_2_tokens, int(is_duplicate)))\n",
    "    print(\"Header: {}\".format(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['what',\n",
       "   'is',\n",
       "   'the',\n",
       "   'step',\n",
       "   'by',\n",
       "   'step',\n",
       "   'guide',\n",
       "   'to',\n",
       "   'invest',\n",
       "   'in',\n",
       "   'share',\n",
       "   'market',\n",
       "   'in',\n",
       "   'india',\n",
       "   '?'],\n",
       "  ['what',\n",
       "   'is',\n",
       "   'the',\n",
       "   'step',\n",
       "   'by',\n",
       "   'step',\n",
       "   'guide',\n",
       "   'to',\n",
       "   'invest',\n",
       "   'in',\n",
       "   'share',\n",
       "   'market',\n",
       "   '?'],\n",
       "  0),\n",
       " (['what',\n",
       "   'is',\n",
       "   'the',\n",
       "   'story',\n",
       "   'of',\n",
       "   'kohinoor',\n",
       "   '(',\n",
       "   'koh',\n",
       "   '-',\n",
       "   'i',\n",
       "   '-',\n",
       "   'noor',\n",
       "   ')',\n",
       "   'diamond',\n",
       "   '?'],\n",
       "  ['what',\n",
       "   'would',\n",
       "   'happen',\n",
       "   'if',\n",
       "   'the',\n",
       "   'indian',\n",
       "   'government',\n",
       "   'stole',\n",
       "   'the',\n",
       "   'kohinoor',\n",
       "   '(',\n",
       "   'koh',\n",
       "   '-',\n",
       "   'i',\n",
       "   '-',\n",
       "   'noor',\n",
       "   ')',\n",
       "   'diamond',\n",
       "   'back',\n",
       "   '?'],\n",
       "  0),\n",
       " (['how',\n",
       "   'can',\n",
       "   'i',\n",
       "   'increase',\n",
       "   'the',\n",
       "   'speed',\n",
       "   'of',\n",
       "   'my',\n",
       "   'internet',\n",
       "   'connection',\n",
       "   'while',\n",
       "   'using',\n",
       "   'a',\n",
       "   'vpn',\n",
       "   '?'],\n",
       "  ['how',\n",
       "   'can',\n",
       "   'internet',\n",
       "   'speed',\n",
       "   'be',\n",
       "   'increased',\n",
       "   'by',\n",
       "   'hacking',\n",
       "   'through',\n",
       "   'dns',\n",
       "   '?'],\n",
       "  0),\n",
       " (['why',\n",
       "   'am',\n",
       "   'i',\n",
       "   'mentally',\n",
       "   'very',\n",
       "   'lonely',\n",
       "   '?',\n",
       "   'how',\n",
       "   'can',\n",
       "   'i',\n",
       "   'solve',\n",
       "   'it',\n",
       "   '?'],\n",
       "  ['find',\n",
       "   'the',\n",
       "   'remainder',\n",
       "   'when',\n",
       "   '[',\n",
       "   'math]23^{24}[/math',\n",
       "   ']',\n",
       "   'is',\n",
       "   'divided',\n",
       "   'by',\n",
       "   '24,23',\n",
       "   '?'],\n",
       "  0),\n",
       " (['which',\n",
       "   'one',\n",
       "   'dissolve',\n",
       "   'in',\n",
       "   'water',\n",
       "   'quikly',\n",
       "   'sugar',\n",
       "   ',',\n",
       "   'salt',\n",
       "   ',',\n",
       "   'methane',\n",
       "   'and',\n",
       "   'carbon',\n",
       "   'di',\n",
       "   'oxide',\n",
       "   '?'],\n",
       "  ['which', 'fish', 'would', 'survive', 'in', 'salt', 'water', '?'],\n",
       "  0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first few lines of the cleaned lines\n",
    "cleaned_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now pickle the output cleaned, tokenized file.\n",
    "pickle.dump(cleaned_lines, open(\"./data/processed/01.processed_train.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read the pickle.\n"
     ]
    }
   ],
   "source": [
    "# As a sanity-check, read the pickle and compare it to cleaned_list\n",
    "pickled_cleaned_lines = pickle.load(open(\"./data/processed/01.processed_train.pkl\", \"rb\"))\n",
    "assert pickled_cleaned_lines == cleaned_lines\n",
    "print(\"Successfully read the pickle.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
